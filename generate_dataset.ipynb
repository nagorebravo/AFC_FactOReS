{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Checking Data Generation\n",
    "\n",
    "This notebook will generate synthetic data for building a fact checking dataser based on the Maldita dataset. \n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "This notebook requires `pip install -e requirements_minimal.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from factchecking_api import (\n",
    "    answer_questions,\n",
    "    generate_article,\n",
    "    generate_metadata,\n",
    "    generate_questions_and_searches,\n",
    ")\n",
    "from src.api_calls.api_manager import ApiManager\n",
    "from src.citation.cication_manager import CitationManager\n",
    "from src.pipelines.rag import RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API KEYS\n",
    "\n",
    "In this notebook we will use OpenAI GPT4o as an LLM and SEPER as a google search results providers. You should set your own API keys. \n",
    "\n",
    "- Get the OpenAI API KEY here: https://platform.openai.com/\n",
    "- Get the Serper API Key here (2500 free searches): https://serper.dev/api-key\n",
    "\n",
    "The API also support the following LLMs\n",
    "- `gpt3`, `gpt4`, `gpt4o`, `gpt4omini` from OpenAI\n",
    "- `haiku`, `sonnet`, `opus` from Claude\n",
    "- `gemini-flash`, `gemini-flash-thinking`, `gemini-pro` from Google\n",
    "- `llama3-8b`, `llama3-70B`, `llama3-405b` via Groq\n",
    "- `deepseek`from Deepseek-Chat\n",
    "\n",
    "It supports the following embedding models\n",
    "- `openai_embeddings_small`, `openai_embeddings_large` from OpenAI\n",
    "- `cohere_embeddings`, `cohere_embeddings_light`, `cohere_rerank_english`, `cohere_rerank_multilingual` from Cohere\n",
    "- `NV-Embed`, `E5-Multilingual-Large`, `E5-Mistral` using the Sentence Transformers library (loads the model in a GPU)\n",
    "\n",
    "It supports the following web search results providers:\n",
    "- `serper` Google search results, cheap and fast: https://serper.dev/\n",
    "- `serpapi` same as serper but more expensive: https://serpapi.com\n",
    "- `you` another search engine https://you.com\n",
    "- `local_google` searches in google locally, free but google will eventually detect that you are robot and refuse your requests\n",
    "- `newsapi` only news https://newsapi.org\n",
    "\n",
    "If you want to implement new models, modify the code, or fix anything there is a `.py` file for each API in `src/api_calls/*.py`. if you want to check if the API is running correctly in your environment you can run the tests is `src/tests/test_*.py`. If you want to run the tests remember to export the API keys to the path using `export OPEN_API_KEY=\"\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These API KEYS are REQUIRED to run the notebook\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"SERPER_API_KEY\"] = \"\"\n",
    "\n",
    "# These API KEYS are NOT REQUIRED to run the notebook, set them if you want to experiment with other models\n",
    "os.environ['SerpAPI_KEY'] = \"\" # https://serpapi.com/dashboard\n",
    "os.environ['YOU_API_KEY'] =  \"\" # https://api.you.com\n",
    "os.environ['ANTHROPIC_API_KEY'] = \"\" # https://console.anthropic.com/settings/keys\n",
    "os.environ['COHERE_API_KEY'] = \"\" # https://dashboard.cohere.com/api-keys\n",
    "os.environ['GROQ_API_KEY'] = \"\" # https://console.groq.com/keys\n",
    "os.environ['NEWS_API_KEY'] = \"\" # https://newsapi.org/docs/get-started#search\n",
    "os.environ['REPLICATE_API_TOKEN'] = \"\" # https://replicate.com/account/api-tokens\n",
    "os.environ['GEMINI_API_KEY'] = \"\" # https://aistudio.google.com/app/apikey\n",
    "os.environ['DEEPSEEK_API_KEY'] = \"\" # https://platform.deepseek.com/api_keys\n",
    "\n",
    "am = ApiManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all the unique claims\n",
    "\n",
    "By running this code we will get all the unique claims in `CONSULTA_PV-Validated-title-edited.json` by `id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = []\n",
    "\n",
    "with open(\"maldita_dataset/CONSULTA_PV-Validated-title-edited.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Get claims\n",
    "\n",
    "print(f\"Total elements: {len(data)}\")\n",
    "data = [x for x in data if x[\"claim\"] is True]\n",
    "print(f\"Total elements that are claims: {len(data)}\")\n",
    "\n",
    "# Remove duplicates by id\n",
    "\n",
    "ids = set()\n",
    "data = [x for x in data if not (x[\"id\"] in ids or ids.add(x[\"id\"]))]\n",
    "print(f\"Total elements that are claims and are unique: {len(data)}\")\n",
    "\n",
    "# Write to file\n",
    "\n",
    "with open(\"maldita_dataset/unique_claims.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all the unique articles\n",
    "\n",
    "By running this code we will get all the unique articles in `CONSULTA_PV-Validated-title-edited.json` by `id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = []\n",
    "\n",
    "with open(\"maldita_dataset/CONSULTA_PV-Validated-title-edited.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Get claims\n",
    "\n",
    "print(f\"Total elements: {len(data)}\")\n",
    "data = [x for x in data if x[\"claim\"] is True]\n",
    "print(f\"Total elements that are claims: {len(data)}\")\n",
    "\n",
    "# Remove duplicates by id\n",
    "\n",
    "ids = set()\n",
    "data = [x for x in data if not (x[\"article_id\"] in ids or ids.add(x[\"article_id\"]))]\n",
    "print(f\"Total articles that are claims and are unique: {len(data)}\")\n",
    "\n",
    "# Write to file\n",
    "\n",
    "with open(\"maldita_dataset/unique_articles.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get questions and searches based on claim\n",
    "\n",
    "For every unique claim in `unique_claims.json` we will retrieve a set of `critical_questions` and `web_searches` that will allow us to verify the claim. We will generate them based in the `claim` field in the dataset. The prompt we will use is defined in `src/prompts/factchecking.py` in `questions_and_searches_prompt`.\n",
    "\n",
    "The process will output the current API cost, estimated API cost and the estimated remaining time. If the process is stoped you can run the cell again and we will resume from the last item that was processed. The cell will generate the file `unique_claims_qs.json`. \n",
    "\n",
    "We use `GPT4o` but you can experiments with any of the other models mentioned above. \n",
    "\n",
    "[MANUAL LABOUR]: Validate the Questions and Searches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"maldita_dataset/unique_claims.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "if os.path.exists(\"maldita_dataset/unique_claims_qs.json\"):\n",
    "    with open(\"maldita_dataset/unique_claims_qs.json\", \"r\", encoding=\"utf8\") as f:\n",
    "        annotated_data = json.load(f)\n",
    "else:\n",
    "    annotated_data = {}\n",
    "\n",
    "print(f\"Total elements: {len(data)}\")\n",
    "print(f\"Already annotated elements: {len(annotated_data)}\")\n",
    "data = data[len(annotated_data) :]\n",
    "print(f\"Elements to annotate: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.reset_cost()\n",
    "with tqdm(data, desc=\"Total cost: $0.0. Estimated cost: $0.0\", total=len(data)) as pbar:\n",
    "    for i, fc in enumerate(data):\n",
    "    \n",
    "        claim_id = fc[\"id\"]\n",
    "        claim = fc[\"new_title\"] \n",
    "            \n",
    "        tqdm.write(f\"\\nClaim: {claim}, ID: {claim_id}\")\n",
    "        searches, questions = generate_questions_and_searches(\n",
    "            am=am,\n",
    "            fact_checking_topic=claim,\n",
    "            model=\"gpt4o\",\n",
    "            language=\"es\",\n",
    "        )\n",
    "        annotated_data[claim_id] = {\n",
    "            \"claim\": claim,\n",
    "            \"searches\": searches,\n",
    "            \"questions\": questions,\n",
    "        }\n",
    "        with open(\"maldita_dataset/unique_claims_qs.json\", \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(annotated_data, f, indent=4, ensure_ascii=False)\n",
    "        total_cost = am.total_cost\n",
    "        estimated_cost = (total_cost / (i + 1)) * len(data)\n",
    "        pbar.set_description(\n",
    "            f\"Total cost: ${total_cost:.2f}. Estimated cost: ${estimated_cost:.2f}\"\n",
    "        )\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get questions and evidences from text\n",
    "\n",
    "*We decided that we will not use this data, so you can skip this*\n",
    "\n",
    "This cell will get from every unique article which questions does that article answer, and which evidence is provided in the article. We will use the prompt defined in the cell below. It will generate the file `unique_articles_eq.json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_ft(fact_checking_article, claim):\n",
    "    return f\"\"\"\n",
    "You will be analyzing a fact-checking article to extract two key elements: evidences and critic questions. \n",
    "\n",
    "The article is about the following claim: \"{claim}\".\n",
    "The article is the following:\n",
    "\n",
    "{fact_checking_article}\n",
    "\n",
    "---\n",
    "\n",
    "First retrieve a list of main evidences from the article. Evidences can support, refute or be neutral to the claim. All the evidences should be extracted from the arcticle. Evidences should be short and concise. Generate as many evidences as you can find in the article, ensure that all the evidences are extracted from the article, do not generate evidences that are not in the article. Evidences should be self-contained and understandable without the need to read the article.\n",
    "\n",
    "Second, retrieve the critic questions that the article uses to challenge the claim. Critic questions are questions that can be asked to challenge the claim. These questions should be critical and aimed at uncovering various aspects of the claim. You should read the article and infer which critical questions the author uses to challenge the claim. These questions might not be explicitly stated in the article, but you should infer them from the content of the article. The questions must be self-contained and understandable without the need to read the article. Questions should be neutral and not biased towards the information already presented in the article. You should generate the questions that the author made to himself before writing the article.\n",
    "\n",
    "Present your response in the following JSON format. The JSON should include three main keys: \"evidences\", \"questions\". \"evidences\" and \"searches\" should be arrays containing the items you generated in each step. \n",
    "\n",
    "\n",
    "Your answer should be in Spanish.\n",
    "\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"maldita_dataset/unique_articles.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "if os.path.exists(\"maldita_dataset/unique_articles_eq.json\"):\n",
    "    with open(\"maldita_dataset/unique_articles_eq.json\", \"r\", encoding=\"utf8\") as f:\n",
    "        annotated_data = json.load(f)\n",
    "else:\n",
    "    annotated_data = {}\n",
    "\n",
    "print(f\"Total elements: {len(data)}\")\n",
    "print(f\"Already annotated elements: {len(annotated_data)}\")\n",
    "data = data[len(annotated_data) :]\n",
    "print(f\"Elements to annotate: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retrieve(BaseModel):\n",
    "    evidences: List[str]\n",
    "    questions: List[str]\n",
    "\n",
    "\n",
    "def retrieve(\n",
    "    am: ApiManager,\n",
    "    fact_checking_article: str,\n",
    "    claim: str,\n",
    "    model: str = \"gpt4o\",\n",
    "):\n",
    "    prompt = get_prompt_ft(fact_checking_article=fact_checking_article, claim=claim)\n",
    "\n",
    "    search_dict = am.structured_completion(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=4096,\n",
    "        pydanctic_model=Retrieve,\n",
    "    )\n",
    "\n",
    "    evidences = search_dict[\"evidences\"]\n",
    "    questions = search_dict[\"questions\"]\n",
    "\n",
    "    return evidences, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.reset_cost()\n",
    "with tqdm(data, desc=\"Total cost: $0.0. Estimated cost: $0.0\", total=len(data)) as pbar:\n",
    "    for i, fc in enumerate(data):\n",
    "        article_id = fc[\"article_id\"]\n",
    "        article = fc[\"content\"]\n",
    "        claim = fc[\"title\"]\n",
    "        evidences, questions = retrieve(\n",
    "            am=am,\n",
    "            fact_checking_article=article,\n",
    "            claim=claim,\n",
    "            model=\"gpt4o\",\n",
    "        )\n",
    "        annotated_data[article_id] = {\n",
    "            \"claim\": claim,\n",
    "            \"article\": article,\n",
    "            \"evidences\": evidences,\n",
    "            \"questions\": questions,\n",
    "        }\n",
    "        with open(\"maldita_dataset/unique_articles_eq.json\", \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(annotated_data, f, indent=4, ensure_ascii=False)\n",
    "        total_cost = am.total_cost\n",
    "        estimated_cost = (total_cost / (i + 1)) * len(data)\n",
    "        pbar.set_description(\n",
    "            f\"Total cost: ${total_cost:.2f}. Estimated cost: ${estimated_cost:.2f}\"\n",
    "        )\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve context\n",
    "\n",
    "Run the web searches and RAG pipelines. For every `article` we have a set of `questions` and `searches`. We will\n",
    "- Retrieve from google search the top_5 web result for every `search`\n",
    "- Split the webpages by `\\n` characters to generate chunks\n",
    "- Generate an embedding for each `question` and `chunk`\n",
    "- Rank the `chunks` by the similarity to the `questions`. \n",
    "\n",
    "Similar to before, you can stop the process and resume it latter. This process will generate the `unique_claims_qs_context.json` file. In this file, for each `question` we will store all the `chunks` ranked by similarity from more similar to less similar. Latter you should decide how many chunks you will use and get the top_k ones. \n",
    "\n",
    "[MANUAL LABOUR] Validate a set of top_k chunks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"maldita_dataset/unique_claims_qs.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    data_all = json.load(f)\n",
    "\n",
    "if os.path.exists(\"maldita_dataset/unique_claims_qs_context.json\"):\n",
    "    with open(\"maldita_dataset/unique_claims_qs_context.json\", \"r\", encoding=\"utf8\") as f:\n",
    "        annotated_data = json.load(f)\n",
    "else:\n",
    "    annotated_data = {}\n",
    "\n",
    "annotated_keys = annotated_data.keys()\n",
    "all_keys = data_all.keys()\n",
    "\n",
    "remaining_keys = set(all_keys) - set(annotated_keys)\n",
    "\n",
    "# Get remaining data\n",
    "data = {k: data_all[k] for k in remaining_keys}\n",
    "\n",
    "print(f\"Total elements: {len(data_all)}\")\n",
    "print(f\"Already annotated elements: {len(annotated_data)}\")\n",
    "print(f\"Elements to annotate: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_context(\n",
    "    am: ApiManager,\n",
    "    searches: List[str],\n",
    "    questions: List[str],\n",
    "    top_k_websites: int, # How many websites to retrieve for each search\n",
    "    embedding_model: str,\n",
    "    web_search_provider: str,\n",
    "    chunk_size: str, # Minimun chunk size for the RAG model. We will generate chunks by splitting the websites by newlines, but each chunk will have at least this size, we wont split until we reach this size.\n",
    "    language: str, # Language of the search, \"es\" for spanish\n",
    "    location: str = None, # Location of the search, \"es\" for retrieving results as if the user was in Spain\n",
    "):\n",
    "    rag = RAG(api_manager=am)\n",
    "\n",
    "    rag.search_questions(\n",
    "        searches,\n",
    "        top_k_websites,\n",
    "        web_search_provider,\n",
    "        language,\n",
    "        location,\n",
    "    )\n",
    "\n",
    "    rag_results = rag.generate_w_ranks(\n",
    "        embedding_model,\n",
    "        chunk_size,\n",
    "        16,\n",
    "        questions,\n",
    "        512,\n",
    "    )\n",
    "\n",
    "    return rag_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.reset_cost()\n",
    "with tqdm(data, desc=\"Total cost: $0.0. Estimated cost: $0.0\", total=len(data)) as pbar:\n",
    "    for i, (key, elem) in enumerate(data.items()):\n",
    "        claim = elem[\"claim\"]\n",
    "        searches = elem[\"searches\"]\n",
    "        questions = elem[\"questions\"]\n",
    "\n",
    "        context = retrieve_relevant_context(\n",
    "            am=am,\n",
    "            searches=searches,\n",
    "            questions=questions,\n",
    "            top_k_websites=5,\n",
    "            embedding_model=\"openai_embeddings_large\",\n",
    "            web_search_provider=\"serper\",\n",
    "            chunk_size=128,\n",
    "            language=\"es\",\n",
    "            location=\"es\",\n",
    "        )\n",
    "\n",
    "        elem[\"context\"] = context\n",
    "        annotated_data[key] = elem\n",
    "        with open(\"maldita_dataset/unique_claims_qs_context.json\", \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(annotated_data, f, indent=4, ensure_ascii=False)\n",
    "        total_cost = am.total_cost\n",
    "        estimated_cost = (total_cost / (i + 1)) * len(data)\n",
    "        pbar.set_description(\n",
    "            f\"Total cost: ${total_cost:.2f}. Estimated cost: ${estimated_cost:.2f}\"\n",
    "        )\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer questions and get article\n",
    "\n",
    "Generate synthetic answers for each question based on the retrieved context. And generate a fact checking with three paragraphs \"supporting evidence\", \"counter evidence\" and \"summary\" based on all the retrieved context. \n",
    "\n",
    "The model will generate citations for every claim.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"maldita_dataset/unique_claims_qs_context.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "if os.path.exists(\"maldita_dataset/unique_claims_qs_context_synthetic.json\"):\n",
    "    with open(\"maldita_dataset/unique_claims_qs_context_synthetic.json\", \"r\", encoding=\"utf8\") as f:\n",
    "        annotated_data = json.load(f)\n",
    "else:\n",
    "    annotated_data = {}\n",
    "\n",
    "annotated_keys = annotated_data.keys()\n",
    "all_keys = data.keys()\n",
    "\n",
    "remaining_keys = set(all_keys) - set(annotated_keys)\n",
    "\n",
    "# Get remaining data\n",
    "data = {k: data[k] for k in remaining_keys}\n",
    "\n",
    "print(f\"Total elements: {len(data)}\")\n",
    "print(f\"Already annotated elements: {len(annotated_data)}\")\n",
    "print(f\"Elements to annotate: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_text(url, rag_results):\n",
    "    for q in rag_results:\n",
    "        for i in range(len(rag_results[q])):\n",
    "            if rag_results[q][i][\"metadata\"][\"url\"] == url:\n",
    "                return rag_results[q][i][\"text\"]\n",
    "    return \"\"\n",
    "\n",
    "def run_async(func, *args, **kwargs):\n",
    "    return asyncio.create_task(asyncio.to_thread(func, *args, **kwargs))\n",
    "\n",
    "\n",
    "def generate_factcheking_response(\n",
    "    cmg: CitationManager,\n",
    "    fact_checking: str,\n",
    "    qa_results: List[str],\n",
    "    metadata: Dict[str, str],\n",
    "    rag_results: Dict[str, List[Dict[str, str]]],\n",
    "):\n",
    "    #print(fact_checking)\n",
    "    fixed_texts, citations = cmg.reorder_citations(\n",
    "        [fact_checking] + [answer for _, answer in qa_results]\n",
    "    )\n",
    "    #print(citations)\n",
    "\n",
    "    fact_checking = fixed_texts[0]\n",
    "    qa_results = {\n",
    "        question: fixed_texts[i + 1] for i, (question, _) in enumerate(qa_results)\n",
    "    }\n",
    "\n",
    "    citations_dict = {\n",
    "        n: {\n",
    "            \"url\": \"\".join(x.url.split(\"?\"))[:-1],\n",
    "            \"source\": x.source,\n",
    "            \"favicon\": x.favicon,\n",
    "            \"text\": find_text(x.url, rag_results),\n",
    "        }\n",
    "        for n, x in zip(citations, cmg.get_metadata(citations))\n",
    "    }\n",
    "\n",
    "    response = {\n",
    "        \"automated_factchecking\": fact_checking,\n",
    "        \"sources\": citations_dict,\n",
    "        \"questions_answers\": qa_results,\n",
    "        \"metadata\": metadata,\n",
    "    }\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "async def generate_data(\n",
    "    am: ApiManager,\n",
    "    rag_results,\n",
    "):\n",
    "    uniqueid = 0\n",
    "    for q in rag_results:\n",
    "        for i in range(len(rag_results[q])):\n",
    "            rag_results[q][i][\"metadata\"][\"url\"] = (\n",
    "                rag_results[q][i][\"metadata\"][\"url\"] + f\"?{uniqueid}\"\n",
    "            )\n",
    "            uniqueid += 1\n",
    "\n",
    "    # print(json.dumps(rag_results,ensure_ascii=False, indent=4))\n",
    "\n",
    "    metadatas = [\n",
    "        snippet[\"metadata\"] for snippets in rag_results.values() for snippet in snippets\n",
    "    ]\n",
    "\n",
    "    # print(metadatas)\n",
    "    cmg = CitationManager(metadatas=metadatas)\n",
    "\n",
    "    qa_task = run_async(answer_questions, am, cmg, rag_results, \"es\", \"gpt4omini\")\n",
    "\n",
    "    article_task = run_async(\n",
    "        generate_article,\n",
    "        am,\n",
    "        cmg,\n",
    "        rag_results,\n",
    "        \"es\",\n",
    "        claim,\n",
    "        \"gpt4o\",\n",
    "        None,\n",
    "    )\n",
    "\n",
    "    qa_results = await qa_task\n",
    "    fact_checking = await article_task\n",
    "\n",
    "    metadata = generate_metadata(\n",
    "        am,\n",
    "        claim,\n",
    "        fact_checking,\n",
    "        \"gpt4omini\",\n",
    "        \"es\",\n",
    "        \"es\",\n",
    "    )\n",
    "\n",
    "    response = generate_factcheking_response(\n",
    "        cmg=cmg,\n",
    "        fact_checking=fact_checking,\n",
    "        qa_results=qa_results,\n",
    "        metadata=metadata,\n",
    "        rag_results=rag_results,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "top_k_chunks = 3\n",
    "\n",
    "am.reset_cost()\n",
    "with tqdm(data, desc=\"Total cost: $0.0. Estimated cost: $0.0\", total=len(data)) as pbar:\n",
    "    for i, (key, elem) in enumerate(data.items()):\n",
    "        cmg = CitationManager()\n",
    "        elem_tmp = deepcopy(elem)\n",
    "        claim = elem_tmp[\"claim\"]\n",
    "        questions = elem_tmp[\"questions\"]\n",
    "        rag_results = elem_tmp[\"context\"]\n",
    "\n",
    "        for q in rag_results:\n",
    "            rag_results[q] = rag_results[q][:top_k_chunks]\n",
    "\n",
    "        response = await generate_data(am, rag_results)\n",
    "\n",
    "        elem[\"synthetic_factchecking\"] = response\n",
    "        annotated_data[key] = elem\n",
    "\n",
    "        with open(\"maldita_dataset/unique_claims_qs_context_synthetic.json\", \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(annotated_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        total_cost = am.total_cost\n",
    "        estimated_cost = total_cost / (i + 1) * len(data)\n",
    "        pbar.set_description(\n",
    "            f\"Total cost: ${total_cost:.2f}. Estimated cost: ${estimated_cost:.2f}\"\n",
    "        )\n",
    "        pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
